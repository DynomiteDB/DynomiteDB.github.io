---
title: "Replication"
slug: "replication"
date: "2016-02-07T20:22:33-08:00"
product: "Dynomite"
version: "v0.5.6"
type: "dynomite-v0.5.6"
docurl: "/docs/dynomite/v0.5.6/"
docsection: "Architecture"
weight: 500
draft: false

---

Replication means that multiple copies of each key/value pair are stored across multiple nodes in a cluster. Each copy of a key/value pair is called a replica. Each replica is stored on a different node where each replica node is in a different rack.

Replication ensures that your application will continue to work when a server, rack or entire data center (DC) fails. In other words, replication is a core component of high availability (HA).

# Replicas and replica nodes

A replica node is a node that owns the data token range of a key/value pair's data token. A data token is generated by hashing the key portion of a key/value pair with a consistent hashing algorithm. Each replica (i.e. copy of a key/value pair) is written to all replica nodes across a cluster.   

DynomiteDB has a masterless, peer-to-peer architecture which means that each replica node is equal to every other replica node. This fact is important as a server or rack failure that affects one replica node has no effect the other replica nodes.

## Data token ownership

<img class="img-responsive center-block"
     style="width: 50%;"
     src="/img/dynomite/v0.5.6/token-ownership.png"
     alt="Token ownership">
<p class="dyno-image-caption text-center">Data token ownership</p>

The diagram above focuses on a single rack with three servers. `s1`'s node token is 0, `s2`'s node token is 1431655765 and `s3`'s node token is 2863311530.

We can determine which nodes are replica nodes for a given key/value pair as follows: 

1. Calculate the data token by hashing the key portion of the key/value pair with a consistent hashing algorithm
2. For each node set the data token ownership range from `nodeToken` to `nextNodeToken - 1`
3. Check if the data token is within the range of data tokens owned by each node

To continue the example in the diagram above we will determine the replica node for a key/value pair with data token = 100 and another key/value pair with data token = 4000000000.

`s1` owns all data tokens from 0 to 1431655764. Therefore, a key/value pair with a data token = 100 will be written to node `s1`. 

`s3` owns all data tokens from 2863311530 to 4294967295. Therefore, a key/value pair with a data token =  4000000000 will be written to node `s3`.

> Don't worry too much about the readability of the data tokens as it is an implementation detail managed internally by DynomiteDB.

Importantly, an application developer never sees or thinks about data tokens. Instead, an application developer focuses on key/value pairs that make sense for the given application, such as `firstName: Jane` or `age: 23`. DynomiteDB's use of data tokens is an implementation detail that is not exposed to the application layer.

# Replication factor (RF)

- SHOW TOPOLOGY DIAGRAM OF 1 DC, 3 RACKS, 3 SERVERS PER RACK

Replication factor (RF) determines how many replicas exist per DC. The RF for a DC is equal to the number of racks in a DC. For example, if a DC has 3 racks then we know that `RF = 3` which means that there are 3 replicas of each key/value pair.

Each rack contains the entire token range from 0 to 4,294,967,295. Each node within the rack is assigned a node token within the token range where all nodes put together own the entire token range. Therefore, a data token is always owned by exactly one node within each rack which explains why the number of racks per DC determines the RF.  

# Replication

Replication happens on write and is a mix of synchronous and asynchronous operations. The number of nodes involved in the synchronous portion of the write is determined by the write consistency level (CL). If the write CL = `dc_one`, then one replica node within the local DC is written to synchronously while all other replica nodes are written to asynchronously. If the write CL = `dc_quorum`, then a quorum (i.e. a simple majority) of replica nodes within the local DC are written to synchronously while all other replica nodes are written to asynchronously. Replica nodes in remote DCs are always written to asynchronously.

In the next few sections we will discuss a few variations of replication:

- Coordinator's role in replication
- Symmetric replication: When all servers across multiple racks are of equal size and number
- Asymmetric replication: When the number and size of servers differ across racks
- Cross-DC replication: Replication from a local DC to one or more remote DCs

## Coordinator's role in replication

- Show diagram of topology with flow lines from client to coordinator to replica node in one rack client to coordinator to a replica node

The diagram above shows how a write request flows from a client to Dynomite to a backend. Importantly, the client (which is typically an API server) uses a standard Redis client to communicate with the Redis-compatible API on a `dynomite` instance. From the client's perspective, it thinks it is communicating to a single server.

`dynomite` receives the request which is passed to the coordinator subsystem. The coordinator hashes the key from the key/value pair to generate a data token. The coordinator uses the data token and its knowledge of the network topology to determine which nodes are the replica nodes for this request.

### Write consistency level (CL) = dc_one

```bash
TODO: SHOW DIAGRAM WITH 1 DC, 3 RACKS, 3 SERVERS PER RACK AND WRITE PATH LINES 
```

When write CL = `dc_one` and the coordinator's node is a replica node, then a write request flows as follows:

- the coordinator synchronously sends the write request to its backend
- the coordinator waits for a response from its backend
- if the coordinator receives a success response from its backend then:
    - the coordinator sends a success response to the client
    - the coordinator asynchronously replicates the write request to all other replica nodes
- if the coordinator receives a failure response from its backend then:
    - the coordinator sends a failure message to the client 

```bash
TODO: SHOW DIAGRAM WITH 1 DC, 3 RACKS, 3 SERVERS PER RACK AND WRITE PATH LINES 
```

When write CL = `dc_one` and the coordinator's node is **not** a replica node, then a write request flows as follows:

- the coordinator synchronously sends the write request to the replica node within its rack
- the coordinator waits for a response from the replica node
- if the coordinator receives a success response from the replica node then:
    - the coordinator sends a success response to the client
    - the coordinator asynchronously replicates the write request to all other replica nodes
- if the coordinator receives a failure response from the replica node then:
    - the coordinator sends a failure message to the client

### Write consistency level (CL) = dc_quorum

```bash
TODO: SHOW DIAGRAM WITH 1 DC, 3 RACKS, 3 SERVERS PER RACK AND WRITE PATH LINES 
```

When write CL = `dc_quorum` and the coordinator's node is a replica node, then a write request flows as follows:

- the coordinator synchronously sends the write request to its backend and enough replica nodes in other racks to obtain a quorum
- the coordinator waits for a response from a quorum of replica nodes
- if the coordinator receives a success response from a quorum of replica nodes then:
    - the coordinator sends a success response to the client
    - the coordinator asynchronously replicates the write request to all other replica nodes
- if the coordinator does not receive a success response from a quorum of replica nodes then:
    - the coordinator sends a failure message to the client

```bash
TODO: SHOW DIAGRAM WITH 1 DC, 3 RACKS, 3 SERVERS PER RACK AND WRITE PATH LINES 
```

When write CL = `dc_quorum` and the coordinator's node is **not** a replica node, then a write request flows as follows:

- the coordinator synchronously sends the write request to enough replica nodes to obtain a quorum
- the coordinator waits for a response from a quorum of replica nodes
- if the coordinator receives a success response from a quorum of replica nodes then:
    - the coordinator sends a success response to the client
    - the coordinator asynchronously replicates the write request to all other replica nodes
- if the coordinator does not receive a success response from a quorum of replica nodes then:
    - the coordinator sends a failure message to the client

### Topology-aware Dyno client

<img class="img-responsive center-block"
     style="width: 50%;"
     src="/img/dynomite/v0.5.6/topology-aware-load-balancing.png"
     alt="Dyno token aware load balancing">
<p class="dyno-image-caption text-center">Topology-aware Dyno client</p>

The Dyno client is topology aware which means that it always sends each request to a replica node. The benefit of this feature is that the coordinator will always be on a replica node which reduces latency and intra-cluster communication.

## Symmetric replication

```bash
TODO: USE DIAGRAM FROM TOPOLOGY WITH REPLICATION LINES. 1 DC, 2 RACKS, 3 SERVERS PER RACK
```

<img class="img-responsive center-block"
     src="/img/dynomite/v0.5.6/topology-aware-local-writes.png"
     alt="Dyno writes to local DC">

The diagram above shows symmetric replication as the single DC has three racks where each rack has three equal size servers (i.e. same CPU, memory, disk, etc.).

A server in each rack owns the same data token range as a server in the other two racks. The table below shows each of the three data token ranges and the three nodes (one from each rack) that owns the range.

<table class="table table-condensed table-bordered">
    <tr class="active">
        <th>Node token</th>
        <th>Min data token</th>
        <th>Max data token</th>
        <th>Replica nodes</th>
    </tr>
    <tr>
        <td>0</td>
        <td>0</td>
        <td>1431655764</td>
        <td>r1s1, r2s1, r3s1</td>
    </tr>
        <td>1431655765</td>
        <td>1431655765</td>
        <td>2863311529</td>
        <td>r1s2, r2s2, r3s2</td>
    </tr>
        <td>2863311530</td>
        <td>2863311530</td>
        <td>4294967295</td>
        <td>r1s3, r2s3, r3s3</td>
    </tr>
</table>

When the client sends a write request for a key/value pair with data token = 3000000000 with write CL = `dc_one`, then the data is written synchronously to one node and asynchronously to the other two replica nodes. 

## Asymmetric replication

```bash
TODO: USE DIAGRAM FROM TOPOLOGY WITH REPLICATION LINES. 1 DC. 2 RACKS. 3 SERVERS IN R1, 6 SERVERS IN R2
```

<img class="img-responsive center-block"
     src="/img/dynomite/v0.5.6/topology-aware-multi-dc-local-writes.png"
     alt="Dyno writes to local DC in multi-DC environment">

The diagram above shows asymmetric replication as the single DC has two racks where one rack has three servers and the other rack has six servers.

Each node in rack `r1` owns 1/3 of the data token range as shown in the table below.

<table class="table table-condensed table-bordered">
    <tr class="active">
        <th>Node token</th>
        <th>Min data token</th>
        <th>Max data token</th>
        <th>Replica nodes</th>
    </tr>
    <tr>
        <td>0</td>
        <td>0</td>
        <td>1431655764</td>
        <td>r1s1</td>
    </tr>
        <td>1431655765</td>
        <td>1431655765</td>
        <td>2863311529</td>
        <td>r1s2</td>
    </tr>
        <td>2863311530</td>
        <td>2863311530</td>
        <td>4294967295</td>
        <td>r1s3</td>
    </tr>
</table>

Each node in rack `r2` owns 1/6 of the data token range as shown in the table below.

<table class="table table-condensed table-bordered">
    <tr class="active">
        <th>Node token</th>
        <th>Min data token</th>
        <th>Max data token</th>
        <th>Replica nodes</th>
    </tr>
    <tr>
        <td>0</td>
        <td>0</td>
        <td>715827881</td>
        <td>r2s1</td>
    </tr>
        <td>715827882</td>
        <td>715827882</td>
        <td>1431655764</td>
        <td>r2s2</td>
    </tr>
        <td>1431655765</td>
        <td>1431655765</td>
        <td>2147483646</td>
        <td>r2s3</td>
    </tr>
    </tr>
        <td>2147483647</td>
        <td>2147483647</td>
        <td>2863311529</td>
        <td>r2s4</td>
    </tr>
    </tr>
        <td>2863311530</td>
        <td>2863311530</td>
        <td>3579139411</td>
        <td>r2s5</td>
    </tr>
    </tr>
        <td>3579139412</td>
        <td>3579139412</td>
        <td>4294967295</td>
        <td>r2s6</td>
    </tr>
</table>

When the client sends a write request for a key/value pair with data token = 3000000000 with write CL = `dc_one`, then the data is written synchronously to `r1s3` and asynchronously to `r2s5`. 

## Multi-DC replication

```bash
SHOW IMAGE OF MULTI-DC TOPOLOGY WITH REPLICATION LINES DRAWN
```

<img class="img-responsive center-block"
     src="/img/dynomite/v0.5.6/cross-dc-replication.png"
     alt="Cross data center (DC) replication">

The diagram above show a cluster with a two DCs that each contain three racks with three servers per rack. The servers are all equal size. Therefore this example shows symmetric multi-DC replication.

Multi-DC replication is nearly identical to the replication within a single DC with multiple racks. During multi-DC replication, the coordinator in the local DC picks a coordinator in the remote DC. The local coordinator then forwards the request to the remote coordinator asynchronously. The remote coordinator then forwards the request to all replica nodes in the remote DC.

The local coordinator uses a remote coordinator to reduce cross-DC traffic. in our example, the remote DC has three replicas, yet only a single request was sent over the network from the local coordinator to the remote coordinator. The remote coordinator is then responsible for all communication within its DC.
 
# Summary

DynomiteDB replicates each key/value pair across multiple servers and racks. A copy of a key/value pair is called a replica. Replicas are stored on nodes that own the data token range that matches the replica's data token. The data token is generated by hashing the key portion of a key/value pair with a consistent hashing algorithm.

Each rack owns the complete token range. The nodes within a rack each own a portion of the range. As a result, each key/value pair is stored on one node per rack.

The number of racks per DC defines the replication factor (RF). The RF is the number of replicas per DC.

During a write request, the write consistency level (CL) determines how many replica nodes are written to synchronously, while the remainder are written to asynchronously. Writes to replica nodes in remote DCs are always written to asynchronously.

DynomiteDB supports both symmetric replication and asymmetric replication across racks and DCs.
